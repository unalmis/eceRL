{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e859f17-06b8-4cd4-a7f7-27c589bc6c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 23:40:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   25C    P8              17W / 300W |      2MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Installing dm_control...\n",
      "env: MUJOCO_GL=egl\n",
      "Checking that the dm_control installation succeeded...\n",
      "Installed dm_control 1.0.18\n"
     ]
    }
   ],
   "source": [
    "#@title Run to install MuJoCo and `dm_control`\n",
    "import distutils.util\n",
    "import os\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "# # This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "# # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "# # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "# NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "# if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "#   with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "#     f.write(\"\"\"{\n",
    "#     \"file_format_version\" : \"1.0.0\",\n",
    "#     \"ICD\" : {\n",
    "#         \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "#     }\n",
    "# }\n",
    "# \"\"\")\n",
    "\n",
    "print('Installing dm_control...')\n",
    "!pip install -q dm_control>=1.0.18\n",
    "\n",
    "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "print('Checking that the dm_control installation succeeded...')\n",
    "try:\n",
    "  from dm_control import suite\n",
    "  env = suite.load('cartpole', 'swingup')\n",
    "  pixels = env.physics.render()\n",
    "except Exception as e:\n",
    "  raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "else:\n",
    "  del pixels, suite\n",
    "\n",
    "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f5067c-9b51-4fcf-9c00-e54a0d41f27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.\n",
      "<frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.\n"
     ]
    }
   ],
   "source": [
    "#@title All `dm_control` imports required for this tutorial\n",
    "\n",
    "# The basic mujoco wrapper.\n",
    "from dm_control import mujoco\n",
    "\n",
    "# Access to enums and MuJoCo library functions.\n",
    "from dm_control.mujoco.wrapper.mjbindings import enums\n",
    "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
    "\n",
    "# PyMJCF\n",
    "from dm_control import mjcf\n",
    "\n",
    "# Composer high level imports\n",
    "from dm_control import composer\n",
    "from dm_control.composer.observation import observable\n",
    "from dm_control.composer import variation\n",
    "\n",
    "# Imports for Composer tutorial example\n",
    "from dm_control.composer.variation import distributions\n",
    "from dm_control.composer.variation import noises\n",
    "from dm_control.locomotion.arenas import floors\n",
    "\n",
    "# Control Suite\n",
    "from dm_control import suite\n",
    "\n",
    "# Run through corridor example\n",
    "from dm_control.locomotion.walkers import cmu_humanoid\n",
    "from dm_control.locomotion.arenas import corridors as corridor_arenas\n",
    "from dm_control.locomotion.tasks import corridors as corridor_tasks\n",
    "\n",
    "# Soccer\n",
    "from dm_control.locomotion import soccer\n",
    "\n",
    "# Manipulation\n",
    "from dm_control import manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5fc513-9f1f-4ae6-a9ab-44ee583af9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "#from pyvirtualdisplay import Display\n",
    "#from IPython import display as disp\n",
    "import copy\n",
    "from typing import Tuple\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23520939-0648-40f4-b94b-db32096ce5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c5b5128-ce05-432b-9e50-aa66cbdd358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_flags():\n",
    "\n",
    "    flags = {\n",
    "        \"env\": \"hopper\",\n",
    "        \"task\": \"hop\",\n",
    "        \"seed\":0, # random seed\n",
    "        \"start_timesteps\": 25e3, #total steps of free exploration phase\n",
    "        \"max_timesteps\": 8e4, # maximum length of time steps in training\n",
    "        \"expl_noise\": 0.1, # noise strength in exploration\n",
    "        \"batch_size\": 512,\n",
    "        \"discount\":0.99,\n",
    "        \"tau\": 0.005, # rate of target update\n",
    "        #\"policy_noise\": 0.2, # policy noise when sampling action\n",
    "        #\"noise_clip\":0.5, # noise clip rate\n",
    "        \"policy_freq\": 2, # delayed policy update frequency in TD3,\n",
    "        \"N\": 1, # number of agents,\n",
    "        \"RR\": 4, # replay ratio,\n",
    "        \"T\": np.inf, # time steps between agent resets ,\n",
    "        \"beta\": 50, # action selection coefficient\n",
    "    }\n",
    "\n",
    "    return flags\n",
    "\n",
    "def collect_actions(theta, state):\n",
    "    actions = []\n",
    "    for theta_i in theta:\n",
    "      action, entropy, mean, vari = (theta_i.select_action(np.array(state)))\n",
    "      actions.append(torch.from_numpy(action))\n",
    "    return actions\n",
    "\n",
    "def main(policy_name = 'DDPG') -> list:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    policy_name: str, the method to implement\n",
    "    Output:\n",
    "    evaluations: list, the reward in every episodes\n",
    "    Call DDPG/TD3 trainer and\n",
    "    \"\"\"\n",
    "    args = init_flags()\n",
    "    random_state = np.random.RandomState(args[\"seed\"])\n",
    "    env = suite.load(args[\"env\"], args[\"task\"], task_kwargs={'random': random_state})\n",
    "    \n",
    "    action_spec = env.action_spec()\n",
    "    action_dim = action_spec.shape[0]\n",
    "    max_action = action_spec.maximum # be careful that max_action is an array!\n",
    "\n",
    "    ob_spec = env.observation_spec()\n",
    "    state_dim = 0\n",
    "    for item in ob_spec:\n",
    "        state_dim += ob_spec[item].shape[0]\n",
    "    \n",
    "    # env = gym.make(args[\"env\"])\n",
    "    # env.seed(args[\"seed\"]+100)\n",
    "    # env.action_spec.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "\n",
    "    # state_dim = env.observation_space.shape[0]\n",
    "    # action_dim = env.action_space.shape[0]\n",
    "    # max_action = float(env.action_space.high[0])\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"max_action\": max_action,\n",
    "        \"discount\": args[\"discount\"],\n",
    "        \"tau\": args[\"tau\"],}\n",
    "    if policy_name == \"TD3\":\n",
    "        # Target policy smoothing is scaled wrt the action scale\n",
    "        #kwargs[\"policy_noise\"] = args[\"policy_noise\"] * max_action\n",
    "        #kwargs[\"noise_clip\"] = args[\"noise_clip\"] * max_action\n",
    "        kwargs[\"policy_freq\"] = args[\"policy_freq\"]\n",
    "        theta = [TD3(**kwargs) for _ in range(args[\"N\"])]\n",
    "        # policy = theta[0]\n",
    "    elif policy_name == \"DDPG\":\n",
    "        policy = DDPG(**kwargs)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "    evaluations = []\n",
    "    actions_l = []\n",
    "    entropies = []\n",
    "    means = []\n",
    "    varis = []\n",
    "    # state, done = env.reset(), False\n",
    "    time_step = env.reset()\n",
    "    state = []\n",
    "    for item in time_step.observation:\n",
    "        for ob in time_step.observation[item]:\n",
    "            state.append(ob)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "    k = 0\n",
    "\n",
    "    for t in range(int(args[\"max_timesteps\"])):\n",
    "\n",
    "      episode_timesteps += 1\n",
    "\n",
    "      # Select action randomly or according to policy\n",
    "      entropy = 0\n",
    "      mean = 0\n",
    "      vari = 0\n",
    "      if t < args[\"start_timesteps\"]:\n",
    "        # action = env.action_space.sample()\n",
    "          action = np.random.uniform(action_spec.minimum,\n",
    "                             action_spec.maximum,\n",
    "                             size=action_spec.shape)\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "        \n",
    "          actions = collect_actions(theta, state)\n",
    "          #print(state)\n",
    "          #print(type(state[0]))\n",
    "          #print(actions)\n",
    "          #print(type(actions))\n",
    "          # compute Q's, then apply softmax\n",
    "          q_sa = torch.hstack(\n",
    "              [theta[k].critic.Q1(torch.FloatTensor(state), action.to(torch.float32)) for action in actions]\n",
    "          )\n",
    "          # dim\n",
    "          max_q_sa, _ = torch.max(q_sa, dim=0)\n",
    "          alpha = args[\"beta\"] / max_q_sa\n",
    "          p_select = F.softmax(q_sa / alpha)\n",
    "        \n",
    "          if (t == args[\"start_timesteps\"]):\n",
    "            print(p_select)\n",
    "\n",
    "          # print(actions)\n",
    "          # print(torch.hstack(actions).numpy())\n",
    "          # action = np.random.choice(a=torch.hstack(actions).numpy(), p=p_select.numpy())\n",
    "          idx = np.arange(0, len(actions))\n",
    "          action_idx = np.random.choice(a=idx, p=p_select.numpy())\n",
    "          #action_idx = np.atleast_1d(action)\n",
    "          action = actions[action_idx].numpy()\n",
    "\n",
    "      # Perform action\n",
    "      time_step = env.step(action)\n",
    "      next_state = []\n",
    "      for item in time_step.observation:\n",
    "          for ob in time_step.observation[item]:\n",
    "              next_state.append(ob)\n",
    "      reward = time_step.reward\n",
    "      done = time_step.last()\n",
    "      \n",
    "      # done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "      done_bool = float(done) if episode_timesteps < 1000 else 0\n",
    "\n",
    "      #print(\"state: \", state)\n",
    "      #print(\"action: \", action)\n",
    "      #print(\"next_state: \", next_state)\n",
    "      #print(\"reward: \", reward)\n",
    "      #print(\"done_bool: \", done_bool)\n",
    "      #print(episode_timesteps)\n",
    "\n",
    "      # Store data in replay buffer\n",
    "      replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "      state = next_state\n",
    "      actions_l.append(action)\n",
    "      entropies.append(entropy)\n",
    "      means.append(mean)\n",
    "      varis.append(vari)\n",
    "      episode_reward += reward\n",
    "\n",
    "      # Train agent after collecting sufficient data\n",
    "      if t >= args[\"start_timesteps\"]:\n",
    "        for j in range(args[\"RR\"]):\n",
    "          for theta_i in theta:\n",
    "            theta_i.train(replay_buffer, args[\"batch_size\"])\n",
    "\n",
    "        #if (t % (args[\"T\"] / args[\"N\"])) == 0:\n",
    "        #    print(k)\n",
    "            # reset just actor or both?\n",
    "        #    theta[k].actor.reset()\n",
    "        #    theta[k].critic.reset()\n",
    "        #    k = (k + 1) % args[\"N\"]\n",
    "\n",
    "      if done:\n",
    "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        evaluations.append(episode_reward)\n",
    "        # print('a', actions_l)\n",
    "        # print('e', entropies)\n",
    "        # print('mu', means)\n",
    "        # print('var', varis)\n",
    "        entropies = []\n",
    "        actions_l = []\n",
    "        means = []\n",
    "        varis = []\n",
    "        # Reset environment\n",
    "        # state, done = env.reset(), False\n",
    "        time_step = env.reset()\n",
    "        state = []\n",
    "        for item in time_step.observation:\n",
    "            for ob in time_step.observation[item]:\n",
    "                state.append(ob)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "\n",
    "    return evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "193ab912-225b-4def-94bd-6300e46a8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Construct the actor/critic network for TD3\n",
    "class Actor_TD3(nn.Module):\n",
    "\tdef __init__(self, state_dim: int, action_dim: int, max_action: float):\n",
    "\t\tsuper(Actor_TD3, self).__init__()\n",
    "\t\t############################\n",
    "\t\t# YOUR IMPLEMENTATION HERE #\n",
    "\t\tself.l1 = nn.Linear(state_dim, 1024)\n",
    "\t\tself.l2 = nn.Linear(1024, 1024)\n",
    "\t\tself.l3 = nn.Linear(1024, 2 * action_dim)\n",
    "\t\t############################\n",
    "\t\tself.max_action = torch.from_numpy(max_action).to(torch.float32)\n",
    "\t\tself.action_dim = action_dim\n",
    "\n",
    "\tdef forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "\t\t############################\n",
    "\t\t# YOUR IMPLEMENTATION HERE #\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\n",
    "\t\t# Hint: Use torch.distributions.Normal\n",
    "\t\ta = self.l3(a)\n",
    "\n",
    "\t\tmean = self.max_action * torch.tanh(a[:,:self.action_dim])\n",
    "\t\tcov = nn.functional.softplus(a[:,self.action_dim:]) + 1e-9\n",
    "        \n",
    "\t\treturn torch.distributions.MultivariateNormal(mean, scale_tril=torch.diag_embed(cov))\n",
    "    ############################\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tfor layer in self.children():\n",
    "\t\t\tif hasattr(layer, \"reset_parameters\"):\n",
    "\t\t\t\tlayer.reset_parameters()\n",
    "\n",
    "\n",
    "class Critic_TD3(nn.Module):\n",
    "\tdef __init__(self, state_dim : int, action_dim: int):\n",
    "\t\tsuper(Critic_TD3, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\t############################\n",
    "\t\t# YOUR IMPLEMENTATION HERE #\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Please implement Q2 below\n",
    "\t\t############################\n",
    "\t\t# YOUR IMPLEMENTATION HERE #\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\t\t############################\n",
    "\n",
    "\tdef forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\t\t############################\n",
    "\t\t# YOUR IMPLEMENTATION HERE #\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\t############################\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "\t\t# print(self.l1.weight.dtype)\n",
    "\t\t# [HINT] only returns q1 for actor update\n",
    "\t\t############################\n",
    "\t\t# YOUR IMPLEMENTATION HERE #\n",
    "\t\tsa = torch.cat([state, action], -1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t  ############################\n",
    "\t\treturn q1\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tfor layer in self.children():\n",
    "\t\t\tif hasattr(layer, \"reset_parameters\"):\n",
    "\t\t\t\tlayer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af433745-2fb3-4b2f-90b3-dd6cce65e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim: int,\n",
    "\t\taction_dim: int,\n",
    "\t\tmax_action: float,\n",
    "\t\tdiscount=0.99,\n",
    "\t\ttau=0.005,\n",
    "\t\t#policy_noise=0.2,\n",
    "\t\t#noise_clip=0.5,\n",
    "\t\tpolicy_freq=2,\n",
    "\t\ttemperature=0.01\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor_TD3(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.critic = Critic_TD3(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.max_action = torch.from_numpy(max_action)\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\t\t#self.policy_noise = policy_noise\n",
    "\t\t#self.noise_clip = noise_clip\n",
    "\t\tself.policy_freq = policy_freq\n",
    "\n",
    "\t\tself.total_it = 0\n",
    "\t\tself.temperature = temperature\n",
    "\n",
    "\n",
    "\tdef select_action(self, state: torch.Tensor) -> torch.Tensor:\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\n",
    "\t\t# mean, std = self.actor(state)\n",
    "\t\t# actor_dist = torch.distributions.Normal(mean, std)\n",
    "        # make std into cov instead?\n",
    "\t\tactor_dist = self.actor(state)\n",
    "    \n",
    "\t\tselected_action = actor_dist.rsample().clamp(-self.max_action,\n",
    "\t\t                                             self.max_action)\n",
    "\t\t# print([actor_dist.batch_shape, actor_dist.event_shape])\n",
    "\t\tentropy = actor_dist.entropy()\n",
    "\t\t# vari = torch.square(std)\n",
    "\t\treturn selected_action.data.numpy().flatten(), entropy.data.numpy(), actor_dist.loc.data.numpy(), actor_dist.covariance_matrix.data.numpy()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=256):\n",
    "\t\tself.total_it += 1\n",
    "\n",
    "\t\t# Sample replay buffer\n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Select action according to the policy,\n",
    "      ############################\n",
    "      # YOUR IMPLEMENTATION HERE #\n",
    "\t\t\t#target_mean, target_std = self.actor_target(\n",
    "\t\t\t#\t\t next_state)\n",
    "\t\t\ttarget_actor_dist = self.actor_target(next_state)\n",
    "\t\t\t#print(\"actor_dist dimensions\", [target_actor_dist.batch_shape, target_actor_dist.event_shape])\n",
    "\t\t\t#target_actor_dist = torch.distributions.Normal(\n",
    "\t\t\t#\t\t target_mean, target_std)\n",
    "\t\t\tnext_action = target_actor_dist.rsample().clamp(\n",
    "\t\t\t\t\t -self.max_action, self.max_action)\n",
    "\t\t\ttarget_entropy = target_actor_dist.entropy().unsqueeze(-1)\n",
    "\t\t\t#print(\"entropy dimensions\", target_entropy.size())\n",
    "      ############################\n",
    "\t\t\t# Compute the target Q value\n",
    "\t\t\ttarget_Q1, target_Q2 = self.critic_target(\n",
    "\t\t\t\t\t next_state, next_action.to(torch.float32))\n",
    "\n",
    "      ############################\n",
    "      # YOUR IMPLEMENTATION HERE #\n",
    "\t\t\t# 1.Calculate the min of two target Q-functions\n",
    "\t\t\t# 2. Calculate the TD target\n",
    "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)#.squeeze(-1)\n",
    "\t\t\t#print(\"reward dim: \", reward.size())\n",
    "\t\t\ttarget_Q = not_done * self.discount * (target_Q + (self.temperature * target_entropy))\n",
    "\n",
    "            #target_Q = reward + not_done * self.discount * (\n",
    "\t\t\t#\t\t target_Q + (self.temperature * target_entropy))\n",
    "\t\t\t#print(\"second target Q dim: \", target_Q.size())\n",
    "\t\t\ttarget_Q = target_Q.detach()\n",
    "      ############################\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.critic(state, action.to(torch.float32))\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "\t\tif self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor loss\n",
    "      ############################\n",
    "      # YOUR IMPLEMENTATION HERE #\n",
    "\t\t\t#mean, std = self.actor(state)\n",
    "\t\t\t#actor_dist = torch.distributions.Normal(\n",
    "\t\t\t#\t\t mean, std)\n",
    "\t\t\tactor_dist = self.actor(state)\n",
    "\t\t\tselected_action = actor_dist.rsample().clamp(\n",
    "\t\t\t\t\t -self.max_action, self.max_action)\n",
    "\n",
    "\t\t\tactor_loss = -(self.critic.Q1(state, selected_action.to(torch.float32)) -\n",
    "\t\t\t (self.temperature * actor_dist.entropy())).mean()\n",
    "      ############################\n",
    "\n",
    "\t\t\t# Optimize the actor\n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "      ############################\n",
    "      # YOUR IMPLEMENTATION HERE #\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\tnew_target_params = self.tau * param.data + (1 - self.tau) * target_param.data\n",
    "\t\t\t\ttarget_param.data.copy_(new_target_params)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\tnew_target_params = self.tau * param.data + (1 - self.tau) * target_param.data\n",
    "\t\t\t\ttarget_param.data.copy_(new_target_params)\n",
    "      ############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df25b30-3484-41b3-b7fd-c96bc50d16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 1000 Episode Num: 1 Episode T: 1000 Reward: 1.049\n",
      "Total T: 2000 Episode Num: 2 Episode T: 1000 Reward: 0.000\n",
      "Total T: 3000 Episode Num: 3 Episode T: 1000 Reward: 0.000\n",
      "Total T: 4000 Episode Num: 4 Episode T: 1000 Reward: 0.000\n",
      "Total T: 5000 Episode Num: 5 Episode T: 1000 Reward: 0.000\n",
      "Total T: 6000 Episode Num: 6 Episode T: 1000 Reward: 0.000\n",
      "Total T: 7000 Episode Num: 7 Episode T: 1000 Reward: 0.000\n",
      "Total T: 8000 Episode Num: 8 Episode T: 1000 Reward: 0.000\n",
      "Total T: 9000 Episode Num: 9 Episode T: 1000 Reward: 0.000\n",
      "Total T: 10000 Episode Num: 10 Episode T: 1000 Reward: 0.000\n",
      "Total T: 11000 Episode Num: 11 Episode T: 1000 Reward: 0.333\n",
      "Total T: 12000 Episode Num: 12 Episode T: 1000 Reward: 0.000\n",
      "Total T: 13000 Episode Num: 13 Episode T: 1000 Reward: 0.000\n",
      "Total T: 14000 Episode Num: 14 Episode T: 1000 Reward: 0.000\n",
      "Total T: 15000 Episode Num: 15 Episode T: 1000 Reward: 0.000\n",
      "Total T: 16000 Episode Num: 16 Episode T: 1000 Reward: 0.000\n",
      "Total T: 17000 Episode Num: 17 Episode T: 1000 Reward: 0.000\n",
      "Total T: 18000 Episode Num: 18 Episode T: 1000 Reward: 0.000\n",
      "Total T: 19000 Episode Num: 19 Episode T: 1000 Reward: 0.000\n",
      "Total T: 20000 Episode Num: 20 Episode T: 1000 Reward: 0.000\n",
      "Total T: 21000 Episode Num: 21 Episode T: 1000 Reward: 0.000\n",
      "Total T: 22000 Episode Num: 22 Episode T: 1000 Reward: 0.000\n",
      "Total T: 23000 Episode Num: 23 Episode T: 1000 Reward: 0.055\n",
      "Total T: 24000 Episode Num: 24 Episode T: 1000 Reward: 0.004\n",
      "Total T: 25000 Episode Num: 25 Episode T: 1000 Reward: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2738720/4230665531.py:123: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p_select = F.softmax(q_sa / alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "Total T: 26000 Episode Num: 26 Episode T: 1000 Reward: 0.000\n",
      "Total T: 27000 Episode Num: 27 Episode T: 1000 Reward: 0.000\n",
      "Total T: 28000 Episode Num: 28 Episode T: 1000 Reward: 0.013\n",
      "Total T: 29000 Episode Num: 29 Episode T: 1000 Reward: 0.000\n",
      "Total T: 30000 Episode Num: 30 Episode T: 1000 Reward: 0.000\n",
      "Total T: 31000 Episode Num: 31 Episode T: 1000 Reward: 0.000\n",
      "Total T: 32000 Episode Num: 32 Episode T: 1000 Reward: 0.000\n",
      "Total T: 33000 Episode Num: 33 Episode T: 1000 Reward: 0.000\n",
      "Total T: 34000 Episode Num: 34 Episode T: 1000 Reward: 0.000\n",
      "Total T: 35000 Episode Num: 35 Episode T: 1000 Reward: 0.000\n",
      "Total T: 36000 Episode Num: 36 Episode T: 1000 Reward: 0.000\n",
      "Total T: 37000 Episode Num: 37 Episode T: 1000 Reward: 0.000\n",
      "Total T: 39000 Episode Num: 39 Episode T: 1000 Reward: 0.000\n"
     ]
    }
   ],
   "source": [
    "evaluation_td3 = main(policy_name = 'TD3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "495e3685-960d-4e4c-94e2-08f43bddd6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0369, -0.4338,  0.353 , -0.5791],\n",
       "       [-0.1975, -0.5541, -0.7902, -0.8466]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_array = [torch.tensor([-0.0369, -0.4338,  0.3530, -0.5791], dtype=torch.float64), torch.tensor([-0.1975, -0.5541, -0.7902, -0.8466], dtype=torch.float64)]\n",
    "np.array([np.array(tensor) for tensor in tensor_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "842d5a12-3318-4141-b299-f3f5171b03d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0369, -0.4338,  0.353 , -0.5791])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_array[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e29d7cca-1ca1-4c63-b153-07602d908a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, len(tensor_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b42f7c85-aed5-4b78-9521-3950f4fe4176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment\n",
    "random_state = np.random.RandomState(42)\n",
    "env = suite.load('hopper', 'run', task_kwargs={'random': random_state})\n",
    "\n",
    "action_spec = env.action_spec()\n",
    "\n",
    "action_spec.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40b8aeab-551e-44cf-8c64-efe94b97a205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1., -1., -1.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec.minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fffdfc2e-1195-4508-b523-e3db16e981f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec.maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09b0b722-1539-4a1c-a74c-df5c2cdfb8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArray(shape=(1,), dtype=dtype('float64'), name=None, minimum=[-1.], maximum=[1.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ceb9463d-d138-4308-a019-faafb8a806e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('position',\n",
       "              Array(shape=(8,), dtype=dtype('float64'), name='position')),\n",
       "             ('velocity',\n",
       "              Array(shape=(9,), dtype=dtype('float64'), name='velocity'))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_spec = env.observation_spec()\n",
    "ob_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0bc80b17-93d1-4b01-9581-6fd18af8c25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim = 0\n",
    "for item in ob_spec:\n",
    "    state_dim += ob_spec[item].shape[0]\n",
    "\n",
    "state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4437ca43-657f-4f70-8782-11eda97f9a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<StepType.FIRST: 0>, reward=None, discount=None, observation=OrderedDict({'position': array([-0.08817184,  0.02524552, -0.08670794, -0.03770115, -0.11239976,\n",
       "        0.00788536,  0.00912473,  0.02224182]), 'velocity': array([ 0.06690488,  0.0184274 , -0.02983502,  0.09082664,  0.04598512,\n",
       "        0.12044083,  0.00073289,  0.114146  ,  0.16024414])}))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action_spec.minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "fe61e948-649c-4cfc-8baa-d20b205a62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.random.uniform(action_spec.minimum,\n",
    "                             action_spec.maximum,\n",
    "                             size=action_spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3a857fe7-4eec-4071-bf80-1206e5aa0341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.97913267, -0.26563407,  0.15866419, -0.7340004 , -0.41211752,\n",
       "        0.30334101])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "28d7ce20-e04f-4f3c-b034-b84c939a87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = env.reset()\n",
    "\n",
    "state = []\n",
    "for item in time_step.observation:\n",
    "    for ob in time_step.observation[item]:\n",
    "        state.append(ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "11c7a61f-7b2b-4d20-8660-29a3c25000a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': array([1, 2, 3]), 'B': array([3, 4, 5])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = {'A':np.array([1,2,3]), 'B':np.array([3,4,5])}\n",
    "#np.concatenate(store.values(),1)\n",
    "#array([1, 2, 3, 3, 4, 5])\n",
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9025d525-1a0e-487e-9818-d16c6ed4bf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('position',\n",
       "              array([-0.0922507 ,  0.0304178 , -0.07834967, -0.02811268, -0.10947232,\n",
       "                      0.00772878, -0.01447208, -0.00515856])),\n",
       "             ('velocity',\n",
       "              array([ 0.10420806,  0.03522159, -0.05837741,  0.13675513,  0.06064862,\n",
       "                      0.19672703,  0.00130005,  0.21453296,  0.28563518]))])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d35ca896-9a87-44b5-a03e-dca023b8b8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values([array([-0.0922507 ,  0.0304178 , -0.07834967, -0.02811268, -0.10947232,\n",
       "        0.00772878, -0.01447208, -0.00515856]), array([ 0.10420806,  0.03522159, -0.05837741,  0.13675513,  0.06064862,\n",
       "        0.19672703,  0.00130005,  0.21453296,  0.28563518])])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step.observation.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bc581f1a-30d9-47d5-8bd1-d46d9e4db400",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The first input argument needs to be a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: The first input argument needs to be a sequence"
     ]
    }
   ],
   "source": [
    "np.concatenate(time_step.observation.values(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ba68bff0-b6be-4dd9-95dc-f5870f11367a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.09225069689751514,\n",
       " 0.0304178012980982,\n",
       " -0.07834967408281786,\n",
       " -0.02811267527586962,\n",
       " -0.1094723188683084,\n",
       " 0.007728775876733688,\n",
       " -0.014472077073197916,\n",
       " -0.0051585564419206215,\n",
       " 0.10420805524039606,\n",
       " 0.03522158744236225,\n",
       " -0.05837741312698571,\n",
       " 0.1367551312020601,\n",
       " 0.060648616548222206,\n",
       " 0.19672703251895046,\n",
       " 0.001300048676622605,\n",
       " 0.21453295703069902,\n",
       " 0.2856351792783388]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e82fde5e-ef72-4e33-abc5-a500a2238363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(time_step.last())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "15a7aeb0-26bd-4878-912a-151f39a87deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011893157750922256"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "50741117-ddb0-45dc-9265-34fe8d57cfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('position',\n",
       "              array([-0.09247594,  0.02837665, -0.08260535, -0.01423998, -0.10560009,\n",
       "                      0.00785399, -0.01488494,  0.00677807])),\n",
       "             ('velocity',\n",
       "              array([ 0.09841373, -0.0225239 , -0.20411492, -0.42556769,  1.38726904,\n",
       "                      0.38722287,  0.0125212 , -0.04128608,  1.19366227]))])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "16b69bdb-e84b-4ad0-81b6-22c1000b9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a116e071-3f35-48a6-97e1-6165a493ac0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<StepType.MID: 1>, reward=0.011893157750922256, discount=1.0, observation=OrderedDict({'position': array([-0.09247594,  0.02837665, -0.08260535, -0.01423998, -0.10560009,\n",
       "        0.00785399, -0.01488494,  0.00677807]), 'velocity': array([ 0.09841373, -0.0225239 , -0.20411492, -0.42556769,  1.38726904,\n",
       "        0.38722287,  0.0125212 , -0.04128608,  1.19366227])}))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981b3f4-2dc6-48a4-b414-98a5f9bddcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, _ = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_final [~/.conda/envs/rl_final/]",
   "language": "python",
   "name": "conda_rl_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
